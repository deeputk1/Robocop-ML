{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1194b6-8c6b-4c88-b30b-4a73277fab2a",
   "metadata": {},
   "source": [
    "<h1>Importing Requirements</h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037d0600-67d4-4b2d-8a4c-a5eb9e32dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "\n",
    "import grpc\n",
    "import sys\n",
    "import socket, os\n",
    "sys.path.append(\"./PyRCRSClient/RCRS_gRPC_Client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43fbaccf-02b2-43be-bf29-249ababf4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecEnv\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3 import PPO,A2C,DQN\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c554d35-c9e0-485f-a51f-ec0d2764e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AgentInfo_pb2\n",
    "import AgentInfo_pb2_grpc\n",
    "import BuildingInfo_pb2\n",
    "import BuildingInfo_pb2_grpc\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.spaces import Discrete, Tuple, Box, MultiDiscrete\n",
    "from gym.utils import seeding\n",
    "\n",
    "import logging, random, socket, pickle, json, subprocess, ast\n",
    "import numpy as np\n",
    "import threading\n",
    "import time, math\n",
    "import signal\n",
    "from subprocess import *\n",
    "from numpy import inf\n",
    "import collections\n",
    "import shutil\n",
    "import pandas as pd\n",
    "def select_state_info_from_action_list(state_info, action_set_list):\n",
    "    state_info = list(state_info)\n",
    "    action_set_list = list(action_set_list)\n",
    "    only_building_info = state_info[0][2::3]\n",
    "    new_state_info = []\n",
    "    for i in range(len(action_set_list)):\n",
    "        if action_set_list[i] in only_building_info:\n",
    "            temp = state_info[0].index(action_set_list[i])\n",
    "            new_state_info.append(state_info[0][temp-2])\n",
    "            new_state_info.append(state_info[0][temp-1])\n",
    "    return new_state_info\n",
    "\n",
    "def run_adf(bid):\n",
    "    # NOTE(gRPC Python Team): .close() is possible on a channel and should be\n",
    "    # used in circumstances in which the with statement does not fit the needs\n",
    "    # of the code.\n",
    "    print(\"client for agents running 1======================================\")\n",
    "\n",
    "    with grpc.insecure_channel('localhost:{}'.format(agent_port)) as channel:\n",
    "        stub = AgentInfo_pb2_grpc.AnimFireChalAgentStub(channel)\n",
    "        response = stub.getAgentInfo(AgentInfo_pb2.ActionInfo(actions = [\n",
    "            # AgentInfo_pb2.Action(agent_id = 210552869, building_id=action_set_list[bid//len_action_list]), \n",
    "            # AgentInfo_pb2.Action(agent_id = 1962675462, building_id=action_set_list[bid%len_action_list])]))\n",
    "\n",
    "            # AgentInfo_pb2.Action(agent_id = 2090075220, building_id=action_set_list[bid_1]), \n",
    "            # AgentInfo_pb2.Action(agent_id = 1618773504, building_id=action_set_list[bid_2]),\n",
    "            # AgentInfo_pb2.Action(agent_id = 1535509101, building_id=action_set_list[bid_3]),\n",
    "            # AgentInfo_pb2.Action(agent_id = 1127234487, building_id=action_set_list[bid_4])]))\n",
    "\n",
    "            # AgentInfo_pb2.Action(agent_id = 2090075220, building_id=action_set_list[bid[0]]), \n",
    "            # AgentInfo_pb2.Action(agent_id = 1618773504, building_id=action_set_list[bid[1]]),\n",
    "            # AgentInfo_pb2.Action(agent_id = 1535509101, building_id=action_set_list[bid[2]]), \n",
    "            # AgentInfo_pb2.Action(agent_id = 1127234487, building_id=action_set_list[bid[3]])]))\n",
    "\n",
    "            #UNCOMMENT FOR UOW\n",
    "            AgentInfo_pb2.Action(agent_id = 549139337, building_id=action_set_list[bid[0]]), AgentInfo_pb2.Action(agent_id = 596851449, building_id=action_set_list[bid[1]])]))\n",
    "            \n",
    "    \n",
    "            # AgentInfo_pb2.Action(agent_id = 210552869, building_id=action_set_list[bid[0]]), AgentInfo_pb2.Action(agent_id = 1962675462, building_id=action_set_list[bid[1]])]))\n",
    "                     \n",
    "    agent_state_info = []\n",
    "\n",
    "    for i in response.agents:\n",
    "        agent_state_info.append(i.agent_id)\n",
    "        agent_state_info.append(i.x)\n",
    "        agent_state_info.append(i.y)\n",
    "        agent_state_info.append(i.water)\n",
    "        agent_state_info.append(i.hp)\n",
    "        agent_state_info.append(i.idle)\n",
    "    print(\"client for agents running 2======================================\", agent_port)\n",
    "    return agent_state_info\n",
    "\n",
    "def run_reward():\n",
    "    with grpc.insecure_channel('localhost:{}'.format(reward_port)) as channel:\n",
    "        stub = BuildingInfo_pb2_grpc.AnimFireChalBuildingStub(channel)\n",
    "        response_reward = stub.getRewards(BuildingInfo_pb2.Empty())\n",
    "    print(\"client for reward running======================================\", reward_port)\n",
    "    return response_reward.reward\n",
    "\n",
    "def run_server():\n",
    "    print(\"client for buildings running 1======================================\", building_port)\n",
    "    with grpc.insecure_channel('localhost:{}'.format(building_port)) as channel:\n",
    "        stub = BuildingInfo_pb2_grpc.AnimFireChalBuildingStub(channel)\n",
    "        response = stub.getBuildingInfo(BuildingInfo_pb2.Empty())\n",
    "    building_state_info = []\n",
    "    for i in response.buildings:\n",
    "        building_state_info.append(i.fieryness)\n",
    "        building_state_info.append(round(i.temperature,2))\n",
    "        building_state_info.append(i.building_id)\n",
    "    print(\"client for buildings running 2======================================\", building_port)\n",
    "    return building_state_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a59be-652c-460e-9533-28bce7efdfea",
   "metadata": {},
   "source": [
    "<h2>Defining Relevant Enviornment Details</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a7cd8e2-2e40-4aea-ac8e-7bc57650827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_port = 20001\n",
    "reward_port = 20002\n",
    "agent_port = 20003\n",
    "MAX_TIMESTEP = 250\n",
    "n_agents  = 2\n",
    "# action_set_list = np.array([55887, 55875, 32789, 26534, 50850, 55870, 56223, 56229, 56214, 55917, 55962, 55898, 33041, 25233, 27992, \n",
    "#                                 33041, 56246, 55859, 55762, 55632, 35851, 39766, 35851, 36181, 42695, 53898, 48470, 35921, 53948, 32688, \n",
    "#                                 55606, 37342, 51857, 45972, 54494, 53426, 32855, 32762, 54508, 24141, 53719, 56395, 53581, 49264, 45054, \n",
    "#                                 52924, 25658, 52940, 57434, 52875, 33514, 55115, 53473, 52674, 52949, 29373, 41923, 51225, 50767, 51583, \n",
    "#                                 44069, 52941, 54540, 57762, 41320, 50628, 25174, 56302, 53998, 55604, 47082, 56160, 32351, 23899, 41346, \n",
    "#                                 55796, 33187, 25322, 57434, 25600, 52768, 52895, 48827, 53472, 24452, 50628, 34587, 44961, 23838, 38143, \n",
    "#                                 54554, 32260, 33514, 37861, 31635, 54625, 49857, 44250, 29873, 42010, 53603])\n",
    "# action_set_list = np.array([255, 960, 905, 934, 935, 936, 937, 298, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, \n",
    "#                    950, 951, 247, 952, 248, 953, 249, 954, 250, 955, 251, 956, 957, 253, 958, 254, 959], dtype = object)\n",
    "\n",
    "#UNCOMMENT FOR UOW MAP\n",
    "action_set_list=np.array([2560,2561,2562,2563,2564,2565,2566,2567,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559])\n",
    "len_action_list = len(action_set_list)\n",
    "hostname = socket.gethostname()\n",
    "# Path \n",
    "path = os.path.join('./Scripts/', hostname) \n",
    "path_for_kill_file = os.path.join('./rcrs-server/Scripts/', \"kill_rcrs.sh\")\n",
    "string_for_launch_file = \"python\" + \" \" + './rcrs-server/Scripts/' + f\"launch_file.py {building_port} {reward_port} {agent_port}\"\n",
    "len_action_list = len(action_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5a9106c-3d9a-41ab-a9ab-8d679e44a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCRSenv(gym.Env):\n",
    "    metadata = {'render.modes' : None}  \n",
    "    current_action = 0\n",
    "    def __init__(self,algo_used,map_used):\n",
    "        self.algo_used=algo_used\n",
    "        self.map_used=map_used\n",
    "        if (self.algo_used == \"PPO2\"):\n",
    "            self.action_space = MultiDiscrete([len(action_set_list)]*n_agents)\n",
    "        else:\n",
    "            self.action_space = Discrete(len_action_list*len_action_list)\n",
    "        low = np.array([-inf]*(len_action_list*2+(6*n_agents)))\n",
    "        high = np.array([inf]*(len_action_list*2+(6*n_agents)))\n",
    "        self.observation_space = Box(low, high, dtype=np.float32, shape=None)\n",
    "        print(\"@#@#@#@#@#@##@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@##@#@\")\n",
    "        print(self.observation_space)\n",
    "        self.curr_episode = 0\n",
    "        self.seed()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        print(\"Step running======================================\")\n",
    "        self.curr_episode += 1\n",
    "        print(self.curr_episode)\n",
    "        state_info_interm = []\n",
    "        state_info_interm.append(run_server())\n",
    "\n",
    "        fieryeness_counter = np.array(state_info_interm[0][0::3])\n",
    "        appending_list = []\n",
    "        for i in fieryeness_counter:\n",
    "            if 0 <= i <= 2:\n",
    "                appending_list.append(float(10/len(fieryeness_counter)))\n",
    "            elif 3 <= i <= 5:\n",
    "                appending_list.append(float(5/len(fieryeness_counter)))\n",
    "            else:\n",
    "                appending_list.append(float(-10/len(fieryeness_counter)))\n",
    "        \n",
    "        self.reward = sum(appending_list)\n",
    "        print(self.reward)\n",
    "        state_info = []\n",
    "        state_info.append(select_state_info_from_action_list(state_info_interm, action_set_list))\n",
    "    # uncomment to run greedy algorithm\n",
    "\n",
    "        # state_info_temp = state_info[0][1::2]\n",
    "        \n",
    "        # action = greedy_actions(state_info_temp, n_agents)\n",
    "        \n",
    "        #action_for_greedy_algo_A1 = int((state_info_temp.index(max(state_info_temp))))\n",
    "        \n",
    "        #maximum=max(state_info_temp[0],state_info_temp[1]) \n",
    "        #secondmax=min(state_info_temp[0],state_info_temp[1]) \n",
    "          \n",
    "        #for i in range(2,len(state_info_temp)): \n",
    "        #    if state_info_temp[i]>maximum: \n",
    "        #        secondmax=maximum\n",
    "        #        maximum=state_info_temp[i] \n",
    "        #    else: \n",
    "        #        if state_info_temp[i]>secondmax: \n",
    "        #            secondmax=state_info_temp[i] \n",
    "\n",
    "        #action_for_greedy_algo_A2 = int((state_info_temp.index(secondmax)))\n",
    "        #action = [action_for_greedy_algo_A1+1, action_for_greedy_algo_A2+1]\n",
    "        # run_reward()\n",
    "        state_info.append(run_adf(action))\n",
    "        # print(\"Action for 210552869\" ,   action_set_list[action[0]])\n",
    "        # print(\"Action for 1618773504\" ,  action_set_list[action[1]])\n",
    "        # print(\"Action for 1535509101\" ,  action_set_list[action[2]])\n",
    "        # print(\"Action for 1127234487\" ,  action_set_list[action[3]])\n",
    "        print(\"-----------------------------------------------\")\n",
    "        flat_list = [item for sublist in state_info for item in sublist]\n",
    "       \n",
    "        self.state = flat_list\n",
    "\n",
    "        done = bool(self.curr_episode == MAX_TIMESTEP)\n",
    "        if done == True:\n",
    "            subprocess.Popen(path_for_kill_file, shell=True)\n",
    "        if (self.map_used == 'Small'):\n",
    "            time.sleep(0.14)\n",
    "        else:\n",
    "            time.sleep(0.19)\n",
    "\n",
    "        return np.array(self.state), self.reward, done , {}\n",
    "\n",
    "    def reset(self):\n",
    "        subprocess.Popen(path_for_kill_file, shell=True)\n",
    "        time.sleep(2)\n",
    "        print(\"Reset running======================================\")\n",
    "        subprocess.Popen(['xterm', '-e', string_for_launch_file])\n",
    "        #time.sleep(10)\n",
    "        # launch_everything(20001,20002,20003)\n",
    "        time.sleep(20)\n",
    "        subprocess.Popen(['xterm', '-e',\"./rcrs-adf-sample/launch.sh '-all -p {}'\".format(agent_port)])\n",
    "\n",
    "        time.sleep(2)\n",
    "        \n",
    "        self.curr_episode = 0\n",
    "        if (self.algo_used == \"PPO2\"):\n",
    "            reset_action = [0]*n_agents\n",
    "        else:\n",
    "            reset_action = 0\n",
    "        reset_interm = []\n",
    "        \n",
    "        print(\"Reset: Agents: Buildings running======================================\")\n",
    "        reset_interm.append(run_server())\n",
    "        reset = []\n",
    "        reset.append(select_state_info_from_action_list(reset_interm, action_set_list))\n",
    "        print(len(reset))\n",
    "        reset.append(run_adf(reset_action))\n",
    "        print(len(reset))\n",
    "        flat_list_reset = [item for sublist in reset for item in sublist]\n",
    "\n",
    "        self.state = flat_list_reset\n",
    "        print(len(self.state),np.array(self.state).shape)\n",
    "        return np.array(self.state) \n",
    "\n",
    "    def seed(self, seed = None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b81fef-9ccb-4f7b-ba9b-c40477777ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ModelSelector(Model):\n",
    "    env=RCRSenv('PPO2','Big')\n",
    "    VecEnv=DummyVecEnv([lambda: env])\n",
    "    VecEnvNorm = VecNormalize(VecEnv, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    if Model=='PPO':\n",
    "        model = PPO(\"MlpPolicy\", VecEnvNorm, verbose=2,n_steps=2)\n",
    "    elif Model=='TRPO':\n",
    "        model = TRPO(\"MlpPolicy\", env, verbose=2,n_steps=2)\n",
    "    else:\n",
    "        model = A2C(\"MlpPolicy\", VecEnvNorm, verbose=2,  n_steps  = 2)\n",
    "    return VecEnvNorm,model\n",
    "def ModelLoader(Model):\n",
    "    env=RCRSenv('PPO2','Big')\n",
    "    VecEnv=DummyVecEnv([lambda: env])\n",
    "    VecEnvNorm = VecNormalize(VecEnv, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    if Model=='PPO':\n",
    "        model = PPO.load(\"./Models/rcrs_wgts_PPO\")\n",
    "    elif Model=='TRPO':\n",
    "        model = TRPO.load(\"./Models/rcrs_wgts_TRPO\")\n",
    "    else:\n",
    "        model = A2C.load(\"./Models/rcrs_wgts_A2C\")\n",
    "    return VecEnvNorm,model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df019a-c7f9-4064-8e1f-a4e9a592204a",
   "metadata": {},
   "source": [
    "<h1>Training..</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cfd9759-b20d-4205-93dd-a8bd053a68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poss_algos=['PPO','TRPO','A2C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d85cb-6c06-46f4-9e10-36741f15f37d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.PPO\n",
      "2.TRPO\n",
      "3.A2C\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the algorithm to train...1/2/3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/D/ashwin_ext/miniconda3/envs/RoboNew/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/D/ashwin_ext/miniconda3/envs/RoboNew/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@#@#@#@#@#@##@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@##@#@\n",
      "Box(-inf, inf, (48,), float32)\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number of episodes 1\n",
      "Enter number of Training Iterations 15\n",
      "Enter number of Testing Iterations 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time : 01:47:39\n",
      "Current pid : \n",
      "Last Pid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./rcrs-server/Scripts/kill_rcrs.sh: line 20: PidArray: bad array subscript\n",
      "./rcrs-server/Scripts/kill_rcrs.sh: line 23: PidArray: bad array subscript\n",
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset running======================================\n",
      "Reset: Agents: Buildings running======================================\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "1\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "2\n",
      "48 (48,)\n",
      "Step running======================================\n",
      "1\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "2\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "---------------------------\n",
      "| time/              |    |\n",
      "|    fps             | 0  |\n",
      "|    iterations      | 1  |\n",
      "|    time_elapsed    | 25 |\n",
      "|    total_timesteps | 2  |\n",
      "---------------------------\n",
      "Step running======================================\n",
      "3\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "4\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 4          |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01179111 |\n",
      "|    clip_fraction        | 0.05       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.78      |\n",
      "|    explained_variance   | -0.104     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 120        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 242        |\n",
      "----------------------------------------\n",
      "Step running======================================\n",
      "5\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "6\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6           |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016260803 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.78       |\n",
      "|    explained_variance   | -0.061      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.057      |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "Step running======================================\n",
      "7\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.999999999999998\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "8\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.722222222222221\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 8          |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01631701 |\n",
      "|    clip_fraction        | 0.05       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.78      |\n",
      "|    explained_variance   | 0.0327     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 109        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0561    |\n",
      "|    value_loss           | 227        |\n",
      "----------------------------------------\n",
      "Step running======================================\n",
      "9\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.722222222222221\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "10\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.722222222222221\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 0          |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 10         |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01714456 |\n",
      "|    clip_fraction        | 0.05       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.78      |\n",
      "|    explained_variance   | 0.0252     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 101        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0556    |\n",
      "|    value_loss           | 212        |\n",
      "----------------------------------------\n",
      "Step running======================================\n",
      "11\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "12\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 12          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026830614 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.0232      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 96.3        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0694     |\n",
      "|    value_loss           | 203         |\n",
      "-----------------------------------------\n",
      "Step running======================================\n",
      "13\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "14\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 14          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030413687 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.0155      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 91.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0726     |\n",
      "|    value_loss           | 193         |\n",
      "-----------------------------------------\n",
      "Step running======================================\n",
      "15\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "Step running======================================\n",
      "16\n",
      "client for buildings running 1====================================== 20001\n",
      "client for buildings running 2====================================== 20001\n",
      "9.444444444444445\n",
      "client for agents running 1======================================\n",
      "client for agents running 2====================================== 20003\n",
      "-----------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 0           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 16          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033343703 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.6        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0735     |\n",
      "|    value_loss           | 179         |\n",
      "-----------------------------------------\n",
      "Current time : 01:48:08\n",
      "Current pid : 1732145 1731700\n",
      "Last Pid: 1731700\n",
      "Current time : 01:48:08\n",
      "Current pid : 1732145\n",
      "Last Pid: 1732145\n",
      "Reset running======================================\n"
     ]
    }
   ],
   "source": [
    "# Name=input('What is your name..')\n",
    "print('1.PPO\\n2.TRPO\\n3.A2C\\n\\n')\n",
    "input_algo=poss_algos[int(input('Enter the algorithm to train...1/2/3'))%3-1]\n",
    "VecEnvNorm,model=ModelSelector(input_algo)\n",
    "episodes=int(input('Enter number of episodes'))\n",
    "training_iterations=int(input('Enter number of Training Iterations'))\n",
    "testing_iterations=int(input('Enter number of Testing Iterations'))\n",
    "\n",
    "final_rewards = []\n",
    "for k in range(episodes):\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=training_iterations)\n",
    "    # Saving the model \n",
    "    subprocess.Popen(path_for_kill_file, shell=True)\n",
    "    obs = VecEnvNorm.reset()\n",
    "    # print(\"testing....\")\n",
    "\t    # Create an empty list to store reward values \n",
    "    for _ in range(testing_iterations):\n",
    "        # predict the values\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = VecEnvNorm.step(action)\n",
    "        if dones == True:\n",
    "            final_rewards.append(rewards)\n",
    "    if dones!=True:\n",
    "        final_rewards.append(rewards)\n",
    "    subprocess.Popen(path_for_kill_file, shell=True)\n",
    "    model.save(\"./Models/{}_{}\".format(\"rcrs_wgts\", input_algo))\n",
    "    subprocess.Popen(path_for_kill_file, shell=True)\n",
    "df = pd.DataFrame(final_rewards,columns=['Rewards'])\n",
    "df.to_csv(\"./Reward_History_per_episode//{}_{}\".format(input_algo, \"Reward.csv\", sep=',',index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7fd11-ab29-4940-9428-231a734af949",
   "metadata": {},
   "source": [
    "<h1>Testing Code..</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbb94b9-1a6c-4e19-96b6-00ce4dc3ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.PPO\n",
      "2.TRPO\n",
      "3.A2C\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the algorithm to test...1/2/3 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@#@#@#@#@#@##@#@#@#@#@#@#@#@#@#@#@#@#@#@#@#@##@#@\n",
      "Box(-inf, inf, (86,), float32)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of iterations.. 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time : 01:34:41\n",
      "Current pid : \n",
      "Last Pid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./rcrs-server/Scripts/kill_rcrs.sh: line 20: PidArray: bad array subscript\n",
      "./rcrs-server/Scripts/kill_rcrs.sh: line 23: PidArray: bad array subscript\n",
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset running======================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Does not exist!!Please train for the selected algorithm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m testing_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter the number of iterations..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mVecEnvNorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(testing_iterations):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# predict the values\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n",
      "File \u001b[0;32m~/miniconda3/envs/RoboNew/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:295\u001b[0m, in \u001b[0;36mVecNormalize.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]]:\n\u001b[1;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    Reset all environments\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    :return: first observation of the episode\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/miniconda3/envs/RoboNew/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RoboNew/lib/python3.9/site-packages/shimmy/openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     warn(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mRCRSenv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mPopen([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxterm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-e\u001b[39m\u001b[38;5;124m'\u001b[39m, string_for_launch_file])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m#time.sleep(10)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# launch_everything(20001,20002,20003)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mPopen([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxterm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-e\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./rcrs-adf-sample/launch.sh \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-all -p \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(agent_port)])\n\u001b[1;32m     93\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('1.PPO\\n2.TRPO\\n3.A2C\\n\\n')\n",
    "input_algo=poss_algos[int(input('Enter the algorithm to test...1/2/3'))%3-1]\n",
    "try:\n",
    "    VecEnvNorm,model=ModelLoader(input_algo)\n",
    "except:\n",
    "    print('Model Does not exist!!Please train for the selected algorithm')\n",
    "testing_iterations=int(input('Enter the number of iterations..'))\n",
    "obs = VecEnvNorm.reset()\n",
    "for _ in range(testing_iterations):\n",
    "    # predict the values\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = VecEnvNorm.step(action)\n",
    "print('Final Score of the system:',rewards[0])\n",
    "subprocess.Popen(path_for_kill_file, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b300b3-ed28-49ba-8c3b-3afb1ad7ca26",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h1>Plotting all results</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05e6c4b4-18b4-42f0-811f-f565e5cd19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4bd09d7d-b9de-4e55-b71b-91019afa5822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for rs in glob.glob('./Reward_History_per_episode/*.csv'):\n",
    "    df_p = pd.read_csv(rs)\n",
    "    plt.plot([i for i in range(len(df_p))],df_p.Rewards)\n",
    "    plt.savefig('./Plots/'+os.path.basename(rs).split('_')[0], facecolor='y', bbox_inches=\"tight\",pad_inches=0.3, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d755f-a01d-4814-b50e-321a8b3116ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots are saved in the the Plots folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
